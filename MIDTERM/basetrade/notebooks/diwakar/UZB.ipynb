{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "import copy\n",
      "\n",
      "import argparse\n",
      "\n",
      "import time\n",
      "from datetime import datetime\n",
      "from pandas.tseries.offsets import BDay\n",
      "\n",
      "from pandas.plotting import autocorrelation_plot\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "from statsmodels.tsa.arima_model import ARIMA\n",
      "import statsmodels.tsa.stattools as ts\n",
      "import statsmodels.api as sm\n",
      "\n",
      "import matplotlib.patches as mpatches\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "\n",
      "\n",
      "sys.path.append(os.path.expanduser('~/dvc-bardata-toolkit/'))\n",
      "\n",
      "import data_utils.data_info as di\n",
      "import signal_utils.signal_info as si\n",
      "\n",
      "\n",
      "from transform_utils.transform_info import make_changes\n",
      "from transform_utils.transform_info import make_ema_changes\n",
      "\n",
      "from datetime_utils.convert_datetime import get_timestamp_tuples\n",
      "from datetime_utils.convert_datetime import get_timestamp_tuples_cumulative\n",
      "from datetime_utils.convert_datetime import get_date_time_from_timestamp\n",
      "\n",
      "import utils.df_utils as ut\n",
      "from utils.trade_utils import TradeGenerator\n",
      "\n",
      "from utils.curve_utils import get_nearing_shortcodes\n",
      "from alphas.curve_alphas import compute_signal_print_trades\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parse(args):\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument('-s', dest='shortcode_list', nargs='+', help='list of shortcodes to be used', required=True)\n",
      "    parser.add_argument('-st', dest='start_time', help='common start time', required=True)\n",
      "    parser.add_argument('-et', dest='end_time', help='common end time', required=True)\n",
      "    parser.add_argument('-sd', dest='start_date', help='common start date', required=False)\n",
      "    parser.add_argument('-ed', dest='end_date', help='common end date', required=False)\n",
      "    parser.add_argument('-sp', dest='signal_params', nargs='+', help='lag of returns duration', required=False)\n",
      "    parser.add_argument('--spread-distance', dest='spread_distance',help=' the distance between two maturities to be considered', required=False)\n",
      "    parser.add_argument('--duration', dest='pred_duration', help='prediction duration ')\n",
      "    parser.add_argument('--trade-type', dest='trade_type', help=' trade type we want SPREAD/FLY', required=True)\n",
      "    return parser.parse_args(args)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "string = '-s UB_0:ZB_0 -st UTC_100 -et UTC_2100 -sd 20160101 -ed 20171006 --spread-distance 3 --trade-type SPREAD --duration 2h -sp 0.012 0.005'\n",
      "arg_str = string.split()\n",
      "\n",
      "args = parse(arg_str)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shortcode_list_to_trade = [shortcode.split(':') for shortcode in args.shortcode_list]\n",
      "\n",
      "list_of_shc_to_trade = [shc for shc_list in shortcode_list_to_trade for shc in shc_list]\n",
      "\n",
      "start_time = args.start_time\n",
      "end_time = args.end_time\n",
      "\n",
      "start_date = int(args.start_date) if args.start_date else 20160101\n",
      "end_date = int(args.end_date) if args.end_date else 20170930\n",
      "\n",
      "signal_name = 'changes'\n",
      "signal_params = ''\n",
      "\n",
      "start_date = datetime.strptime(str(start_date), \"%Y%m%d\")\n",
      "end_date = datetime.strptime(str(end_date), \"%Y%m%d\")\n",
      "\n",
      "num_trade_hours = int(end_time[4:len(end_time)]) - int(start_time[4:len(start_time)])\n",
      "num_trade_hours /= 100.0\n",
      "\n",
      "print('NUM_TRADING_HOURS', num_trade_hours)\n",
      "\n",
      "while start_date.weekday() >= 5:\n",
      "    start_date += BDay(1)\n",
      "    \n",
      "spread_distance = int(args.spread_distance) if args.spread_distance else 1\n",
      "pred_duration = args.pred_duration if args.pred_duration else '1h'\n",
      "\n",
      "threshold_vec = list(map(float,args.signal_params))\n",
      "\n",
      "\n",
      "work_dir = os.path.expanduser(\"~/curve_trading/\") + '_'.join(list_of_shc_to_trade).lower() + '/' \\\n",
      "           + str(spread_distance) + '/' + pred_duration + '/'\n",
      "\n",
      "unique_id = time.time()\n",
      "work_dir += str(int(unique_id*1000))\n",
      "\n",
      "print(work_dir)\n",
      "os.system(\"mkdir --parents \" + work_dir)\n",
      "position_file_prefix = work_dir + \"pos_\"\n",
      "\n",
      "\n",
      "shortcode_list = copy.copy(list_of_shc_to_trade)\n",
      "\n",
      "shortcode_to_neibhors = {}\n",
      "\n",
      "print('Trading Proucts', ' '.join(list_of_shc_to_trade))\n",
      "if args.trade_type == 'FLY':\n",
      "    for shortcode in list_of_shc_to_trade:\n",
      "        higher = get_nearing_shortcodes(shortcode, 1*spread_distance)\n",
      "        lower = get_nearing_shortcodes(shortcode, -1*spread_distance)\n",
      "        if lower and higher:\n",
      "            shortcode_list += [higher, lower]\n",
      "            shortcode_to_neibhors[shortcode] = [lower, higher]\n",
      "elif args.trade_type == 'SPREAD':\n",
      "    if len(list_of_shc_to_trade) % 2 != 0:\n",
      "        print ('ERROR, Need Pairs for trading SPREAD')\n",
      "        \n",
      "shortcode_list = list(set(shortcode_list))\n",
      "\n",
      "print('All Shortcodes', ' '.join(shortcode_list))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_end_keys = get_timestamp_tuples_cumulative(start_date.strftime(\"%Y%m%d\"), end_date.strftime(\"%Y%m%d\"))\n",
      "\n",
      "# get the pairs for each day\n",
      "daywise_start_end = list(get_timestamp_tuples(start_date.strftime('%Y%m%d'), end_date.strftime('%Y%m%d'), start_time, end_time))\n",
      "\n",
      "print('Trying Fetch Data ', ' '.join(shortcode_list), ' For Date ',start_date, ' ', end_date, ' ',\n",
      "      start_time,' ', end_time, ' ', start_end_keys[0], ' ', start_end_keys[1])\n",
      "\n",
      "all_prices_df = di.fetch_data(shortcode_list, ['open_price'], start_end_keys[0], start_end_keys[1]).astype(float)\n",
      "\n",
      "print(daywise_start_end)\n",
      "date = start_date\n",
      "map_prices_df = {}\n",
      "map_date_unix_times = {}\n",
      "count = 0\n",
      "while date <= end_date:\n",
      "    yyyymmdd = date.strftime('%Y%m%d')\n",
      "    # print(type(yyyymmdd))\n",
      "    map_prices_df[yyyymmdd] = pd.DataFrame(columns=all_prices_df.columns)\n",
      "    \n",
      "    # print (count, len(daywise_start_end), date, end_date)\n",
      "    map_date_unix_times[yyyymmdd] = daywise_start_end[count]\n",
      "    map_prices_df[yyyymmdd] = all_prices_df[np.bitwise_and(daywise_start_end[count][0] < all_prices_df.index ,\n",
      "                                            all_prices_df.index < daywise_start_end[count][1])]\n",
      "    \n",
      "    count += 1\n",
      "    date += BDay(1)\n",
      "\n",
      "# for key in map_prices_df.keys():\n",
      "#    print(key, map_prices_df[key].shape)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "while date < end_date:\n",
      "    yyyymmdd = date.strftime('%Y%m%d')\n",
      "\n",
      "    # fetch data for one day\n",
      "    keys = get_timestamp_tuples(date.strftime(\"%Y%m%d\"), date.strftime(\"%Y%m%d\"), start_time, end_time)\n",
      "\n",
      "    for tp in keys:\n",
      "        print('Trying Fetch Data ', ' '.join(shortcode_list), ' For Date ', yyyymmdd)\n",
      "        prices_df = di.fetch_data(shortcode_list, ['open_price'], tp[0], tp[1]).astype(float)\n",
      "        print('Fetched Data ', ' '.join(shortcode_list), ' For Date ', yyyymmdd)\n",
      "\n",
      "        if prices_df.empty:\n",
      "            continue\n",
      "\n",
      "        map_prices_df[yyyymmdd] = prices_df\n",
      "    date += BDay(1)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now we have data for all shortcode(dep/indep) for all days\n",
      "day_to_price_change_df = make_changes(map_prices_df, [pred_duration], '30s', shortcode_list)\n",
      "day_to_ema_change_df = make_ema_changes(map_prices_df, [pred_duration], '30s', shortcode_list)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "columns = ['30s'] + shortcode_list\n",
      "\n",
      "\n",
      "price_change_cumulative_df = pd.concat([day_to_price_change_df[key] for key in sorted(day_to_price_change_df.keys())])\n",
      "ema_change_cumulative_df = pd.concat([day_to_ema_change_df[key] for key in sorted(day_to_price_change_df.keys())])\n",
      "price_cumulative_df = pd.concat([map_prices_df[key] for key in sorted(map_prices_df.keys())])\n",
      "\n",
      "price_cumulative_df.insert(0, '30s',price_cumulative_df.index.to_series())\n",
      "print(price_cumulative_df['30s'].iloc[40])\n",
      "\n",
      "price_change_cumulative_df.columns = columns\n",
      "ema_change_cumulative_df.columns = columns\n",
      "price_cumulative_df.columns = columns\n",
      "\n",
      "price_change_cumulative_df = price_change_cumulative_df.dropna()\n",
      "ema_change_cumulative_df = ema_change_cumulative_df.dropna()\n",
      "price_cumulatiev_df = price_cumulative_df.dropna()\n",
      "\n",
      "\n",
      "price_change_cumulative_df.describe()\n",
      "ema_change_cumulative_df.describe()\n",
      "price_cumulative_df.describe()\n",
      "\n",
      "price_cumulative_df[shortcode_list].plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spread = price_cumulative_df[shortcode_list[0]] - price_cumulative_df[shortcode_list[1]] \n",
      "halflife = 560 # 480\n",
      "spread_ema = spread.ewm(halflife=halflife).mean()\n",
      "\n",
      "mr_corr_filename = work_dir + '/' + '_'.join(shortcode_list) + '_mr_' + str(halflife) + '_lags.txt'\n",
      "\n",
      "mr_corr_file = open(mr_corr_filename, 'w')\n",
      "\n",
      "mr_corr_file.write(str((spread - spread_ema).describe()) + '\\n')\n",
      "\n",
      "for i in range(10, 5000):\n",
      "    # print(i, (spread-spread_ema).autocorr(i))\n",
      "    #non_overlapping_df = (spread - spread_ema).iloc[::i]\n",
      "    #print(i, (non_overlapping_df).autocorr(1))\n",
      "    mr_corr_file.write(str(i) + ' '  + str((spread - spread_ema).autocorr(i)))\n",
      "    \n",
      "mr_corr_file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print(spread.iloc[3000:4000])\n",
      "#print(type(spread))\n",
      "spread.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spread_ema.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(spread-spread_ema).plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lag=100\n",
      "plt.plot((spread.iloc[lag:].as_matrix()-spread.iloc[:-1*lag].as_matrix()))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lag = 100\n",
      "#print((spread.iloc[lag:] - spread.iloc[:-1*lag]))\n",
      "autocorrelation_plot((spread.iloc[lag:].as_matrix()-spread.iloc[:-1*lag].as_matrix())[:300])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "spread_trend = spread - spread_ema\n",
      "\n",
      "train_spread = spread \n",
      "\n",
      "# print (test_length, train_length, spread.shape)\n",
      "\n",
      "# train on 10 days, predict for today\n",
      "moving_window = int(2*num_trade_hours*60*10)\n",
      "index_increment = int(2*num_trade_hours*60)\n",
      "\n",
      "current_position = 0\n",
      "\n",
      "print ('Data Separated', train_spread.shape)\n",
      "for lookback in [180, 120, 80, 40]:\n",
      "\n",
      "    current_index = lookback\n",
      "    trades_filename = work_dir + '/trades_' + '_'.join(shortcode_list).lower() + '_' + str(lookback)\n",
      "    signal_filename = work_dir + '/signal_' + '_'.join(shortcode_list).lower() + '_' + str(lookback)\n",
      "    approx_pnlfilename = work_dir + '/approx_pnl' + '_'.join(shortcode_list).lower() + '_' + str(lookback)\n",
      "    \n",
      "    signal_file = open(signal_filename, 'w')\n",
      "    \n",
      "    trade_generator = TradeGenerator(shortcode_list, trades_filename, approx_pnlfilename,\n",
      "                                     commish=0.14, numbers_to_dollars=1000)\n",
      "    \n",
      "    while current_index < train_spread.shape[0] - moving_window - index_increment:\n",
      "        \n",
      "        # >> print('CI', current_index, 'MV', moving_window, 'LB', lookback)\n",
      "        \n",
      "        # last 10 days data\n",
      "        train_start_index = current_index\n",
      "        train_end_index = current_index + moving_window\n",
      "        \n",
      "        y = (train_spread.iloc[train_start_index:train_end_index].as_matrix() -\n",
      "             train_spread.iloc[train_start_index-lookback:train_end_index-lookback].as_matrix())\n",
      "        \n",
      "        train_timestamps = train_spread.index[train_start_index:train_end_index]\n",
      "        \n",
      "        # todays data\n",
      "        test_start_index = current_index + moving_window + 1\n",
      "        test_end_index = current_index + moving_window + index_increment\n",
      "        \n",
      "        # >> print ('ST', test_start_index, test_end_index, train_start_index, train_end_index)\n",
      "        test_y = (train_spread.iloc[test_start_index:test_end_index].as_matrix() \n",
      "                  - train_spread.iloc[(test_start_index-lookback):(test_end_index-lookback)].as_matrix())\n",
      "        \n",
      "        # compute the timestamps separately to print trades\n",
      "        test_timestamps = train_spread.index[test_start_index:test_end_index]\n",
      "        \n",
      "        # non-ovelapping train and test y\n",
      "        non_overlapping_y = y[::lookback]\n",
      "        non_overlapping_test_y = test_y[::lookback]\n",
      "\n",
      "        # print ('Interval: ', lookback, ' Data Size: Train: ', non_overlapping_y.shape, ' Test: ', non_overlapping_test_y.shape)\n",
      "\n",
      "        current_index += index_increment\n",
      "\n",
      "        predictions = np.array([])\n",
      "        print_string = ''\n",
      "        for idx in range(non_overlapping_test_y.shape[0]):\n",
      "            # print ('ITER', idx, y)\n",
      "            model = ARIMA(non_overlapping_y, order=((1),0,0))\n",
      "            model_fit = model.fit()\n",
      "\n",
      "            prediction = model_fit.forecast()[0]\n",
      "            observation = non_overlapping_test_y[idx]\n",
      "\n",
      "            predictions = np.append(predictions, prediction)\n",
      "\n",
      "            non_overlapping_y = np.append(non_overlapping_y, observation)\n",
      "\n",
      "            ctime = test_timestamps[idx]\n",
      "            time_list = [price_cumulative_df.loc[ctime][shortcode_list[0]], price_cumulative_df.loc[ctime][shortcode_list[1]]]\n",
      "                \n",
      "            if prediction >= threshold_vec[0] and current_position != 1:\n",
      "                trade_generator.print_trades(ctime, [20, -20], time_list)\n",
      "                trade_generator.compute_pnl(ctime, [20, -20], time_list)\n",
      "                current_position = 1\n",
      "            elif prediction <= -1*threshold_vec[0] and current_position != -1:\n",
      "                ctime = test_timestamps[idx]\n",
      "                trade_generator.print_trades(ctime, [-20, 20], time_list)\n",
      "                trade_generator.compute_pnl(ctime, [-20, 20], time_list)\n",
      "                current_position = -1\n",
      "            elif prediction < threshold_vec[1] and prediction > -1*threshold_vec[1] and current_position != 0:\n",
      "                trade_generator.print_trades(ctime, [0, 0], time_list)\n",
      "                trade_generator.compute_pnl(ctime, [0, 0], time_list)\n",
      "                current_position = 0\n",
      "                \n",
      "            print_string += ('predicted=%f, expected=%f\\n' % (prediction, observation))\n",
      "            \n",
      "        # print (model_fit.summary())\n",
      "        print_string += ('ARPARAMS %d %f\\n' %( lookback, model_fit.arparams))\n",
      "        \n",
      "        print_string += ('M/STD RESID %d %f %f\\n' % (lookback, np.std(model_fit.resid), np.mean(model_fit.resid)))\n",
      "\n",
      "        print_string += ('MSE %d %f\\n' % (lookback, np.mean(predictions-non_overlapping_test_y)**2))\n",
      "        print_string += ('COR %d %f\\n' %( lookback, np.corrcoef(predictions, non_overlapping_test_y)[0][1]))\n",
      "        signal_file.write(print_string)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(train_spread.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}